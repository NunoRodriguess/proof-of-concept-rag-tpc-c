{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Notebook\n",
    "This notebook loads a JSONL file containing entries with `query`, `gold_answer`, and `predicted_answer`, and computes evaluation metrics:\n",
    "- Exact Match (EM)\n",
    "- Numeric tolerance match (absolute + relative)\n",
    "- Token-level F1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "def load_jsonl(path: str) -> List[Dict]:\n",
    "    data = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    return text.strip().lower() if isinstance(text, str) else str(text)\n",
    "\n",
    "def exact_match(gold, pred):\n",
    "    return normalize(gold) == normalize(pred)\n",
    "\n",
    "def numeric_match(gold, pred, abs_tol=1e-6, rel_tol=1e-3):\n",
    "    try:\n",
    "        g = float(gold)\n",
    "        p = float(pred)\n",
    "        return abs(g - p) <= abs_tol or abs(g - p) / max(abs(g), 1e-9) <= rel_tol\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def token_f1(gold, pred):\n",
    "    g_tokens = normalize(gold).split()\n",
    "    p_tokens = normalize(pred).split()\n",
    "    if len(g_tokens) == 0 or len(p_tokens) == 0:\n",
    "        return 0\n",
    "    overlap = len(set(g_tokens) & set(p_tokens))\n",
    "    if overlap == 0:\n",
    "        return 0\n",
    "    precision = overlap / len(p_tokens)\n",
    "    recall = overlap / len(g_tokens)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def mae_error(gold,pred):\n",
    "        try:\n",
    "            g = float(gold)\n",
    "            p = float(pred)\n",
    "            return abs(p - g)\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "def rmse_component(gold, pred):\n",
    "    \"\"\"\n",
    "    Returns the squared error (p - g)^2 for one gold/pred pair.\n",
    "    Returns None if not numeric.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        g = float(gold)\n",
    "        p = float(pred)\n",
    "        return (p - g) ** 2\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "        \n",
    "def numeric_distances(gold, pred):\n",
    "    \"\"\"\n",
    "    Returns (abs_distance, relative_distance) for one gold/pred pair.\n",
    "    Returns (None, None) if not numeric.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        g = float(gold)\n",
    "        p = float(pred)\n",
    "        abs_dist = abs(p - g)\n",
    "        rel_dist = abs_dist / (abs(g) + 1e-9)\n",
    "        return abs_dist, rel_dist\n",
    "    except:\n",
    "        return None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee109be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(numeric_match(\"25.0\", \"25.0\"))  # Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2780bbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Basic parser: convert whitespace-separated table into columns\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def parse_table_to_columns(text, expected_cols=None):\n",
    "    \"\"\"\n",
    "    Parse a whitespace-separated table into column lists.\n",
    "\n",
    "    Features:\n",
    "    - If expected_cols is provided, the parser will reshape rows to that width.\n",
    "    - Supports optional headers (removed if detected).\n",
    "    - Returns: {col_index: [values...]}\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return {}\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "    rows = []\n",
    "    current = []\n",
    "\n",
    "    for tok in tokens:\n",
    "        current.append(tok)\n",
    "        # If expected_cols known → force row breaks\n",
    "        if expected_cols and len(current) == expected_cols:\n",
    "            rows.append(current)\n",
    "            current = []\n",
    "\n",
    "    # Leftover row (only if we don't have expected_cols)\n",
    "    if current and not expected_cols:\n",
    "        rows.append(current)\n",
    "\n",
    "    def looks_like_header(row):\n",
    "        # Header = mostly strings\n",
    "        string_count = sum(1 for x in row if not is_numeric(x))\n",
    "        return string_count >= len(row) * 0.7\n",
    "\n",
    "    if rows and looks_like_header(rows[0]):\n",
    "        header = rows.pop(0)  # drop it\n",
    "    else:\n",
    "        header = None\n",
    "\n",
    "    if expected_cols:\n",
    "        norm_rows = []\n",
    "        for r in rows:\n",
    "            if len(r) < expected_cols:\n",
    "                r = r + [None] * (expected_cols - len(r))\n",
    "            elif len(r) > expected_cols:\n",
    "                r = r[:expected_cols]\n",
    "            norm_rows.append(r)\n",
    "        rows = norm_rows\n",
    "\n",
    "    columns = {}\n",
    "    for r in rows:\n",
    "        for ci, val in enumerate(r):\n",
    "            columns.setdefault(ci, []).append(val)\n",
    "\n",
    "    return columns\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Type detection\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def is_numeric(x):\n",
    "    try:\n",
    "        float(x)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Numeric distance (MAE)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def numeric_distance(gold_vals, pred_vals):\n",
    "    \"\"\"\n",
    "    Compute mean absolute error between shared numeric entries.\n",
    "    Rows beyond the predicted table length are ignored.\n",
    "    Missing predicted values → penalty via None handling.\n",
    "    \"\"\"\n",
    "    n = min(len(gold_vals), len(pred_vals))\n",
    "    if n == 0:\n",
    "        return None\n",
    "\n",
    "    errs = []\n",
    "    for g, p in zip(gold_vals[:n], pred_vals[:n]):\n",
    "        try:\n",
    "            errs.append(abs(float(p) - float(g)))\n",
    "        except:\n",
    "            # Predicted non-numeric for numeric column → heavy penalty\n",
    "            return None\n",
    "\n",
    "    return sum(errs) / len(errs) if errs else None\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. String distance (very simple Levenshtein-like metric)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def string_distance(gold_vals, pred_vals):\n",
    "    \"\"\"\n",
    "    Compute average normalized edit distance between string cells.\n",
    "    Lower is better; normalized to between 0 and 1.\n",
    "    Uses simple character-based Levenshtein distance.\n",
    "    \"\"\"\n",
    "\n",
    "    def lev(a, b):\n",
    "        # classic DP edit distance\n",
    "        dp = [[i + j if i * j == 0 else 0 for j in range(len(b)+1)]\n",
    "              for i in range(len(a)+1)]\n",
    "\n",
    "        for i in range(1, len(a)+1):\n",
    "            for j in range(1, len(b)+1):\n",
    "                dp[i][j] = min(\n",
    "                    dp[i-1][j] + 1,      # delete\n",
    "                    dp[i][j-1] + 1,      # insert\n",
    "                    dp[i-1][j-1] + (a[i-1] != b[j-1])  # replace\n",
    "                )\n",
    "        return dp[-1][-1]\n",
    "\n",
    "    n = min(len(gold_vals), len(pred_vals))\n",
    "    if n == 0:\n",
    "        return None\n",
    "\n",
    "    errs = []\n",
    "    for g, p in zip(gold_vals[:n], pred_vals[:n]):\n",
    "        g = str(g)\n",
    "        p = str(p)\n",
    "        dist = lev(g, p)\n",
    "        denom = max(len(g), 1)\n",
    "        errs.append(dist / denom)   # 0..1 normalized\n",
    "\n",
    "    return sum(errs) / len(errs) if errs else None\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. Simple Column-Based Evaluator\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def evaluate_columns(gold_text, pred_text, expected_cols=None):\n",
    "    \"\"\"\n",
    "    Returns a dict:\n",
    "        {\n",
    "          column_index: {\n",
    "             \"type\": \"numeric\" or \"string\",\n",
    "             \"score\": ...,\n",
    "             \"coverage\": ...\n",
    "          },\n",
    "          ...\n",
    "        }\n",
    "\n",
    "    We compute:\n",
    "    - numeric MAE for numeric columns\n",
    "    - string edit distance for non-numeric columns\n",
    "\n",
    "    Coverage:\n",
    "    = (# predicted rows for that column) / (# gold rows)\n",
    "    \"\"\"\n",
    "\n",
    "    gold_cols = parse_table_to_columns(gold_text, expected_cols)\n",
    "    pred_cols = parse_table_to_columns(pred_text, expected_cols)\n",
    "\n",
    "    all_col_indices = set(gold_cols.keys()) | set(pred_cols.keys())\n",
    "    results = {}\n",
    "\n",
    "    for ci in all_col_indices:\n",
    "        g_vals = gold_cols.get(ci, [])\n",
    "        p_vals = pred_cols.get(ci, [])\n",
    "\n",
    "        # coverage: how much of the gold column is predicted?\n",
    "        coverage = min(len(g_vals), len(p_vals)) / max(len(g_vals), 1)\n",
    "\n",
    "        # detect type: majority numeric in gold column → numeric\n",
    "        numeric_count = sum(is_numeric(v) for v in g_vals)\n",
    "        is_num = numeric_count > len(g_vals) / 2.0\n",
    "\n",
    "        if is_num:\n",
    "            score = numeric_distance(g_vals, p_vals)\n",
    "            col_type = \"numeric\"\n",
    "        else:\n",
    "            score = string_distance(g_vals, p_vals)\n",
    "            col_type = \"string\"\n",
    "\n",
    "        results[ci] = {\n",
    "            \"type\": col_type,\n",
    "            \"coverage\": coverage,\n",
    "            \"score\": score\n",
    "        }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c8022c",
   "metadata": {},
   "source": [
    "## Qwen Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04376f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Exact Match Accuracy: 0.18181818181818182\n",
      "✔ Numeric Match Accuracy: 0.13636363636363635\n",
      "✔ Token-level F1 (avg): 0.14782236596631335\n",
      "✔ MAE (avg): 18007.184825\n",
      "✔ RMSE: 36014.36965\n"
     ]
    }
   ],
   "source": [
    "data = load_jsonl(\"result_qwen.jsonl\")\n",
    "\n",
    "em_scores = []\n",
    "numeric_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "mae_list = []\n",
    "rmse_list = []\n",
    "abs_dist_list = []\n",
    "rel_dist_list = []\n",
    "\n",
    "column_eval_list = []\n",
    "\n",
    "for item in data:\n",
    "    gold = item.get(\"gold\", \"\")\n",
    "    pred = item.get(\"predicted\", \"\")\n",
    "    total_cols = item.get(\"columns\", None)\n",
    "\n",
    "    # basic metrics\n",
    "    em_scores.append(exact_match(gold, pred))\n",
    "    numeric_scores.append(numeric_match(gold, pred))\n",
    "    f1_scores.append(token_f1(gold, pred))\n",
    "\n",
    "    # per-item numeric metrics\n",
    "    mae = mae_error(gold, pred)\n",
    "    rmse = rmse_component(gold, pred)\n",
    "    abs_dist, rel_dist = numeric_distances(gold, pred)\n",
    "\n",
    "    mae_list.append(mae)\n",
    "    rmse_list.append(rmse)\n",
    "    abs_dist_list.append(abs_dist)\n",
    "    rel_dist_list.append(rel_dist)\n",
    "    try:\n",
    "        column_eval = evaluate_columns(gold, pred, total_cols)\n",
    "        column_eval_list.append(column_eval)\n",
    "    except:\n",
    "        column_eval_list.append({})\n",
    "\n",
    "\n",
    "with open(\"qwen_eval_results.txt\", \"w\") as f:\n",
    "    for i, item in enumerate(data):\n",
    "        f.write(f\"Item {i+1}:\\n\")\n",
    "        f.write(f\"Gold: {item.get('gold', '')}\\n\")\n",
    "        f.write(f\"Predicted: {item.get('predicted', '')}\\n\")\n",
    "\n",
    "        f.write(f\"Exact Match: {'✔' if em_scores[i] else '✘'}\\n\")\n",
    "        f.write(f\"Numeric Match: {'✔' if numeric_scores[i] else '✘'}\\n\")\n",
    "        f.write(f\"Token-level F1: {f1_scores[i]:.4f}\\n\")\n",
    "\n",
    "        f.write(f\"MAE (abs error): {mae_list[i]}\\n\")\n",
    "        f.write(f\"RMSE component (sq error): {rmse_list[i]}\\n\")\n",
    "        f.write(f\"Abs Distance: {abs_dist_list[i]}\\n\")\n",
    "        f.write(f\"Rel Distance: {rel_dist_list[i]}\\n\")\n",
    "\n",
    "        f.write(\"Column Evaluation:\\n\")\n",
    "        for col_index, colinfo in column_eval_list[i].items():\n",
    "            if colinfo:  # Ensure colinfo is not empty\n",
    "                f.write(\n",
    "                    f\"  Column {col_index}: \"\n",
    "                    f\"type={colinfo['type']}, \"\n",
    "                    f\"coverage={colinfo['coverage']:.2f}, \"\n",
    "                    f\"score={colinfo['score']}\\n\"\n",
    "                )\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# --------------------\n",
    "# Aggregate summary\n",
    "# --------------------\n",
    "\n",
    "# Filter out None values\n",
    "valid_mae = [x for x in mae_list if x is not None]\n",
    "valid_rmse = [x for x in rmse_list if x is not None]\n",
    "\n",
    "final_mae = sum(valid_mae) / len(valid_mae) if valid_mae else None\n",
    "final_rmse = math.sqrt(sum(valid_rmse) / len(valid_rmse)) if valid_rmse else None\n",
    "\n",
    "print(\"✔ Exact Match Accuracy:\", sum(em_scores) / len(em_scores))\n",
    "print(\"✔ Numeric Match Accuracy:\", sum(numeric_scores) / len(numeric_scores))\n",
    "print(\"✔ Token-level F1 (avg):\", sum(f1_scores) / len(f1_scores))\n",
    "print(\"✔ MAE (avg):\", final_mae)\n",
    "print(\"✔ RMSE:\", final_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b03e58",
   "metadata": {},
   "source": [
    "## Mistral Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fce048f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Exact Match Accuracy: 0.2727272727272727\n",
      "✔ Numeric Match Accuracy: 0.045454545454545456\n",
      "✔ Token-level F1 (avg): 0.0886991667641363\n",
      "✔ MAE (avg): 0.5\n",
      "✔ RMSE: 0.7071067811865476\n"
     ]
    }
   ],
   "source": [
    "data = load_jsonl(\"result_pixtral.jsonl\")\n",
    "\n",
    "em_scores = []\n",
    "numeric_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "mae_list = []\n",
    "rmse_list = []\n",
    "abs_dist_list = []\n",
    "rel_dist_list = []\n",
    "\n",
    "column_eval_list = []\n",
    "\n",
    "for item in data:\n",
    "    gold = item.get(\"gold\", \"\")\n",
    "    pred = item.get(\"predicted\", \"\")\n",
    "    total_cols = item.get(\"columns\", None)\n",
    "\n",
    "    # basic metrics\n",
    "    em_scores.append(exact_match(gold, pred))\n",
    "    numeric_scores.append(numeric_match(gold, pred))\n",
    "    f1_scores.append(token_f1(gold, pred))\n",
    "\n",
    "    # per-item numeric metrics\n",
    "    mae = mae_error(gold, pred)\n",
    "    rmse = rmse_component(gold, pred)\n",
    "    abs_dist, rel_dist = numeric_distances(gold, pred)\n",
    "\n",
    "    mae_list.append(mae)\n",
    "    rmse_list.append(rmse)\n",
    "    abs_dist_list.append(abs_dist)\n",
    "    rel_dist_list.append(rel_dist)\n",
    "    try:\n",
    "        column_eval = evaluate_columns(gold, pred, total_cols)\n",
    "        column_eval_list.append(column_eval)\n",
    "    except:\n",
    "        column_eval_list.append({})\n",
    "\n",
    "\n",
    "with open(\"mistral_eval_results.txt\", \"w\") as f:\n",
    "    for i, item in enumerate(data):\n",
    "        f.write(f\"Item {i+1}:\\n\")\n",
    "        f.write(f\"Gold: {item.get('gold', '')}\\n\")\n",
    "        f.write(f\"Predicted: {item.get('predicted', '')}\\n\")\n",
    "\n",
    "        f.write(f\"Exact Match: {'✔' if em_scores[i] else '✘'}\\n\")\n",
    "        f.write(f\"Numeric Match: {'✔' if numeric_scores[i] else '✘'}\\n\")\n",
    "        f.write(f\"Token-level F1: {f1_scores[i]:.4f}\\n\")\n",
    "\n",
    "        f.write(f\"MAE (abs error): {mae_list[i]}\\n\")\n",
    "        f.write(f\"RMSE component (sq error): {rmse_list[i]}\\n\")\n",
    "        f.write(f\"Abs Distance: {abs_dist_list[i]}\\n\")\n",
    "        f.write(f\"Rel Distance: {rel_dist_list[i]}\\n\")\n",
    "\n",
    "        f.write(\"Column Evaluation:\\n\")\n",
    "        for col_index, colinfo in column_eval_list[i].items():\n",
    "            if colinfo:  # Ensure colinfo is not empty\n",
    "                f.write(\n",
    "                    f\"  Column {col_index}: \"\n",
    "                    f\"type={colinfo['type']}, \"\n",
    "                    f\"coverage={colinfo['coverage']:.2f}, \"\n",
    "                    f\"score={colinfo['score']}\\n\"\n",
    "                )\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# --------------------\n",
    "# Aggregate summary\n",
    "# --------------------\n",
    "\n",
    "# Filter out None values\n",
    "valid_mae = [x for x in mae_list if x is not None]\n",
    "valid_rmse = [x for x in rmse_list if x is not None]\n",
    "\n",
    "final_mae = sum(valid_mae) / len(valid_mae) if valid_mae else None\n",
    "final_rmse = math.sqrt(sum(valid_rmse) / len(valid_rmse)) if valid_rmse else None\n",
    "\n",
    "print(\"✔ Exact Match Accuracy:\", sum(em_scores) / len(em_scores))\n",
    "print(\"✔ Numeric Match Accuracy:\", sum(numeric_scores) / len(numeric_scores))\n",
    "print(\"✔ Token-level F1 (avg):\", sum(f1_scores) / len(f1_scores))\n",
    "print(\"✔ MAE (avg):\", final_mae)\n",
    "print(\"✔ RMSE:\", final_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a215481",
   "metadata": {},
   "source": [
    "## LLama Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "124f8404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Exact Match Accuracy: 0.13636363636363635\n",
      "✔ Numeric Match Accuracy: 0.09090909090909091\n",
      "✔ Token-level F1 (avg): 0.1566753653115568\n",
      "✔ MAE (avg): 20410.172325\n",
      "✔ RMSE: 40799.350054498216\n"
     ]
    }
   ],
   "source": [
    "data = load_jsonl(\"result_llama_mav.jsonl\")\n",
    "\n",
    "em_scores = []\n",
    "numeric_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "mae_list = []\n",
    "rmse_list = []\n",
    "abs_dist_list = []\n",
    "rel_dist_list = []\n",
    "\n",
    "column_eval_list = []\n",
    "\n",
    "for item in data:\n",
    "    gold = item.get(\"gold\", \"\")\n",
    "    pred = item.get(\"predicted\", \"\")\n",
    "    total_cols = item.get(\"columns\", None)\n",
    "\n",
    "    # basic metrics\n",
    "    em_scores.append(exact_match(gold, pred))\n",
    "    numeric_scores.append(numeric_match(gold, pred))\n",
    "    f1_scores.append(token_f1(gold, pred))\n",
    "\n",
    "    # per-item numeric metrics\n",
    "    mae = mae_error(gold, pred)\n",
    "    rmse = rmse_component(gold, pred)\n",
    "    abs_dist, rel_dist = numeric_distances(gold, pred)\n",
    "\n",
    "    mae_list.append(mae)\n",
    "    rmse_list.append(rmse)\n",
    "    abs_dist_list.append(abs_dist)\n",
    "    rel_dist_list.append(rel_dist)\n",
    "    try:\n",
    "        column_eval = evaluate_columns(gold, pred, total_cols)\n",
    "        column_eval_list.append(column_eval)\n",
    "    except:\n",
    "        column_eval_list.append({})\n",
    "\n",
    "\n",
    "with open(\"llama_mav_eval_results.txt\", \"w\") as f:\n",
    "    for i, item in enumerate(data):\n",
    "        f.write(f\"Item {i+1}:\\n\")\n",
    "        f.write(f\"Gold: {item.get('gold', '')}\\n\")\n",
    "        f.write(f\"Predicted: {item.get('predicted', '')}\\n\")\n",
    "\n",
    "        f.write(f\"Exact Match: {'✔' if em_scores[i] else '✘'}\\n\")\n",
    "        f.write(f\"Numeric Match: {'✔' if numeric_scores[i] else '✘'}\\n\")\n",
    "        f.write(f\"Token-level F1: {f1_scores[i]:.4f}\\n\")\n",
    "\n",
    "        f.write(f\"MAE (abs error): {mae_list[i]}\\n\")\n",
    "        f.write(f\"RMSE component (sq error): {rmse_list[i]}\\n\")\n",
    "        f.write(f\"Abs Distance: {abs_dist_list[i]}\\n\")\n",
    "        f.write(f\"Rel Distance: {rel_dist_list[i]}\\n\")\n",
    "\n",
    "        f.write(\"Column Evaluation:\\n\")\n",
    "        for col_index, colinfo in column_eval_list[i].items():\n",
    "            if colinfo:  # Ensure colinfo is not empty\n",
    "                f.write(\n",
    "                    f\"  Column {col_index}: \"\n",
    "                    f\"type={colinfo['type']}, \"\n",
    "                    f\"coverage={colinfo['coverage']:.2f}, \"\n",
    "                    f\"score={colinfo['score']}\\n\"\n",
    "                )\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# --------------------\n",
    "# Aggregate summary\n",
    "# --------------------\n",
    "\n",
    "# Filter out None values\n",
    "valid_mae = [x for x in mae_list if x is not None]\n",
    "valid_rmse = [x for x in rmse_list if x is not None]\n",
    "\n",
    "final_mae = sum(valid_mae) / len(valid_mae) if valid_mae else None\n",
    "final_rmse = math.sqrt(sum(valid_rmse) / len(valid_rmse)) if valid_rmse else None\n",
    "\n",
    "print(\"✔ Exact Match Accuracy:\", sum(em_scores) / len(em_scores))\n",
    "print(\"✔ Numeric Match Accuracy:\", sum(numeric_scores) / len(numeric_scores))\n",
    "print(\"✔ Token-level F1 (avg):\", sum(f1_scores) / len(f1_scores))\n",
    "print(\"✔ MAE (avg):\", final_mae)\n",
    "print(\"✔ RMSE:\", final_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610e29d1",
   "metadata": {},
   "source": [
    "## Gemini Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "451adbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Exact Match Accuracy: 0.2727272727272727\n",
      "✔ Numeric Match Accuracy: 0.13636363636363635\n",
      "✔ Token-level F1 (avg): 0.11419277520972436\n",
      "✔ MAE (avg): 0.0\n",
      "✔ RMSE: 0.0\n"
     ]
    }
   ],
   "source": [
    "data = load_jsonl(\"result_gemini.jsonl\")\n",
    "\n",
    "em_scores = []\n",
    "numeric_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "mae_list = []\n",
    "rmse_list = []\n",
    "abs_dist_list = []\n",
    "rel_dist_list = []\n",
    "\n",
    "column_eval_list = []\n",
    "\n",
    "for item in data:\n",
    "    gold = item.get(\"gold\", \"\")\n",
    "    pred = item.get(\"predicted\", \"\")\n",
    "    total_cols = item.get(\"columns\", None)\n",
    "\n",
    "    # basic metrics\n",
    "    em_scores.append(exact_match(gold, pred))\n",
    "    numeric_scores.append(numeric_match(gold, pred))\n",
    "    f1_scores.append(token_f1(gold, pred))\n",
    "\n",
    "    # per-item numeric metrics\n",
    "    mae = mae_error(gold, pred)\n",
    "    rmse = rmse_component(gold, pred)\n",
    "    abs_dist, rel_dist = numeric_distances(gold, pred)\n",
    "\n",
    "    mae_list.append(mae)\n",
    "    rmse_list.append(rmse)\n",
    "    abs_dist_list.append(abs_dist)\n",
    "    rel_dist_list.append(rel_dist)\n",
    "    try:\n",
    "        column_eval = evaluate_columns(gold, pred, total_cols)\n",
    "        column_eval_list.append(column_eval)\n",
    "    except:\n",
    "        column_eval_list.append({})\n",
    "\n",
    "\n",
    "with open(\"gemini_results.txt\", \"w\") as f:\n",
    "    for i, item in enumerate(data):\n",
    "        f.write(f\"Item {i+1}:\\n\")\n",
    "        f.write(f\"Gold: {item.get('gold', '')}\\n\")\n",
    "        f.write(f\"Predicted: {item.get('predicted', '')}\\n\")\n",
    "\n",
    "        f.write(f\"Exact Match: {'✔' if em_scores[i] else '✘'}\\n\")\n",
    "        f.write(f\"Numeric Match: {'✔' if numeric_scores[i] else '✘'}\\n\")\n",
    "        f.write(f\"Token-level F1: {f1_scores[i]:.4f}\\n\")\n",
    "\n",
    "        f.write(f\"MAE (abs error): {mae_list[i]}\\n\")\n",
    "        f.write(f\"RMSE component (sq error): {rmse_list[i]}\\n\")\n",
    "        f.write(f\"Abs Distance: {abs_dist_list[i]}\\n\")\n",
    "        f.write(f\"Rel Distance: {rel_dist_list[i]}\\n\")\n",
    "\n",
    "        f.write(\"Column Evaluation:\\n\")\n",
    "        for col_index, colinfo in column_eval_list[i].items():\n",
    "            if colinfo:  # Ensure colinfo is not empty\n",
    "                f.write(\n",
    "                    f\"  Column {col_index}: \"\n",
    "                    f\"type={colinfo['type']}, \"\n",
    "                    f\"coverage={colinfo['coverage']:.2f}, \"\n",
    "                    f\"score={colinfo['score']}\\n\"\n",
    "                )\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# --------------------\n",
    "# Aggregate summary\n",
    "# --------------------\n",
    "\n",
    "# Filter out None values\n",
    "valid_mae = [x for x in mae_list if x is not None]\n",
    "valid_rmse = [x for x in rmse_list if x is not None]\n",
    "\n",
    "final_mae = sum(valid_mae) / len(valid_mae) if valid_mae else None\n",
    "final_rmse = math.sqrt(sum(valid_rmse) / len(valid_rmse)) if valid_rmse else None\n",
    "\n",
    "print(\"✔ Exact Match Accuracy:\", sum(em_scores) / len(em_scores))\n",
    "print(\"✔ Numeric Match Accuracy:\", sum(numeric_scores) / len(numeric_scores))\n",
    "print(\"✔ Token-level F1 (avg):\", sum(f1_scores) / len(f1_scores))\n",
    "print(\"✔ MAE (avg):\", final_mae)\n",
    "print(\"✔ RMSE:\", final_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28034dd",
   "metadata": {},
   "source": [
    "## DeepSeel Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "408e457d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Exact Match Accuracy: 0.22727272727272727\n",
      "✔ Numeric Match Accuracy: 0.0\n",
      "✔ Token-level F1 (avg): 0.07319857525055883\n",
      "✔ MAE (avg): 1.0\n",
      "✔ RMSE: 1.0\n"
     ]
    }
   ],
   "source": [
    "data = load_jsonl(\"result_deep.jsonl\")\n",
    "\n",
    "em_scores = []\n",
    "numeric_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "mae_list = []\n",
    "rmse_list = []\n",
    "abs_dist_list = []\n",
    "rel_dist_list = []\n",
    "\n",
    "column_eval_list = []\n",
    "\n",
    "for item in data:\n",
    "    gold = item.get(\"gold\", \"\")\n",
    "    pred = item.get(\"predicted\", \"\")\n",
    "    total_cols = item.get(\"columns\", None)\n",
    "\n",
    "    # basic metrics\n",
    "    em_scores.append(exact_match(gold, pred))\n",
    "    numeric_scores.append(numeric_match(gold, pred))\n",
    "    f1_scores.append(token_f1(gold, pred))\n",
    "\n",
    "    # per-item numeric metrics\n",
    "    mae = mae_error(gold, pred)\n",
    "    rmse = rmse_component(gold, pred)\n",
    "    abs_dist, rel_dist = numeric_distances(gold, pred)\n",
    "\n",
    "    mae_list.append(mae)\n",
    "    rmse_list.append(rmse)\n",
    "    abs_dist_list.append(abs_dist)\n",
    "    rel_dist_list.append(rel_dist)\n",
    "    try:\n",
    "        column_eval = evaluate_columns(gold, pred, total_cols)\n",
    "        column_eval_list.append(column_eval)\n",
    "    except:\n",
    "        column_eval_list.append({})\n",
    "\n",
    "\n",
    "with open(\"deep_results.txt\", \"w\") as f:\n",
    "    for i, item in enumerate(data):\n",
    "        f.write(f\"Item {i+1}:\\n\")\n",
    "        f.write(f\"Gold: {item.get('gold', '')}\\n\")\n",
    "        f.write(f\"Predicted: {item.get('predicted', '')}\\n\")\n",
    "\n",
    "        f.write(f\"Exact Match: {'✔' if em_scores[i] else '✘'}\\n\")\n",
    "        f.write(f\"Numeric Match: {'✔' if numeric_scores[i] else '✘'}\\n\")\n",
    "        f.write(f\"Token-level F1: {f1_scores[i]:.4f}\\n\")\n",
    "\n",
    "        f.write(f\"MAE (abs error): {mae_list[i]}\\n\")\n",
    "        f.write(f\"RMSE component (sq error): {rmse_list[i]}\\n\")\n",
    "        f.write(f\"Abs Distance: {abs_dist_list[i]}\\n\")\n",
    "        f.write(f\"Rel Distance: {rel_dist_list[i]}\\n\")\n",
    "\n",
    "        f.write(\"Column Evaluation:\\n\")\n",
    "        for col_index, colinfo in column_eval_list[i].items():\n",
    "            if colinfo:  # Ensure colinfo is not empty\n",
    "                f.write(\n",
    "                    f\"  Column {col_index}: \"\n",
    "                    f\"type={colinfo['type']}, \"\n",
    "                    f\"coverage={colinfo['coverage']:.2f}, \"\n",
    "                    f\"score={colinfo['score']}\\n\"\n",
    "                )\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# --------------------\n",
    "# Aggregate summary\n",
    "# --------------------\n",
    "\n",
    "# Filter out None values\n",
    "valid_mae = [x for x in mae_list if x is not None]\n",
    "valid_rmse = [x for x in rmse_list if x is not None]\n",
    "\n",
    "final_mae = sum(valid_mae) / len(valid_mae) if valid_mae else None\n",
    "final_rmse = math.sqrt(sum(valid_rmse) / len(valid_rmse)) if valid_rmse else None\n",
    "\n",
    "print(\"✔ Exact Match Accuracy:\", sum(em_scores) / len(em_scores))\n",
    "print(\"✔ Numeric Match Accuracy:\", sum(numeric_scores) / len(numeric_scores))\n",
    "print(\"✔ Token-level F1 (avg):\", sum(f1_scores) / len(f1_scores))\n",
    "print(\"✔ MAE (avg):\", final_mae)\n",
    "print(\"✔ RMSE:\", final_rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
